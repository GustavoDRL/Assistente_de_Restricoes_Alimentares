# ğŸ¥— Assistente de RestriÃ§Ãµes Alimentares

## ğŸŒŸ VisÃ£o Geral
Bem-vindo ao projeto mais saboroso do GitHub! Este Ã© um assistente inteligente desenvolvido com Streamlit que ajuda pessoas com restriÃ§Ãµes alimentares a navegarem pelo complexo mundo da alimentaÃ§Ã£o de forma segura e personalizada.

### ğŸ“¸ Telas do Sistema

#### Tela de Entrada
![Tela de Entrada](img/tela1.png)

#### Interface de Chat
![Interface de Chat](img/tela2.png)

### ğŸ¨ Interface Personalizada
- Onboarding amigÃ¡vel para coletar informaÃ§Ãµes do usuÃ¡rio
- Chat interativo com IA para tirar dÃºvidas
- Perfil personalizado com suas restriÃ§Ãµes alimentares
- Interface responsiva e intuitiva

### ğŸ¤– Assistente Inteligente
- Respostas personalizadas baseadas no seu perfil
- SugestÃµes de substituiÃ§Ãµes de ingredientes
- Alertas sobre contaminaÃ§Ã£o cruzada
- Dicas nutricionais personalizadas

### ğŸ“š Recursos Educacionais
- SeÃ§Ã£o de dicas de seguranÃ§a alimentar
- Links Ãºteis para informaÃ§Ãµes adicionais
- Recursos da ANVISA e outras organizaÃ§Ãµes relevantes

## ğŸ› ï¸ Tecnologias Utilizadas
- **Streamlit**: Para interface web interativa
- **LangChain**: Para integraÃ§Ã£o com LLM
- **Ollama**: Modelo de linguagem para respostas inteligentes
- **Python Dataclasses**: Para estruturaÃ§Ã£o de dados
- **Logging**: Para rastreamento de eventos

## ğŸš€ Como ComeÃ§ar

### Configurando o Ambiente de IA

#### 1. InstalaÃ§Ã£o do Ollama
Primeiro, vocÃª precisa instalar o Ollama, que gerenciarÃ¡ o modelo de IA localmente:

**Linux e macOS**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

**Windows**
- Acesse [ollama.com/download](https://ollama.com/download)
- Baixe e execute o instalador Windows
- Siga as instruÃ§Ãµes de instalaÃ§Ã£o

#### 2. ConfiguraÃ§Ã£o do Modelo Llama
ApÃ³s instalar o Ollama, configure o modelo Llama 3.1:

```bash
# Download do modelo
ollama pull llama3.1

# Verifique a instalaÃ§Ã£o
ollama list
```

#### 3. VerificaÃ§Ã£o do ServiÃ§o
```bash
# O serviÃ§o Ollama deve estar rodando em segundo plano
# EndereÃ§o padrÃ£o: localhost:11434

# Para verificar o status (Linux/macOS):
systemctl status ollama

# Para iniciar manualmente se necessÃ¡rio:
ollama serve
```

**Nota**: Mantenha o serviÃ§o Ollama rodando enquanto usar a aplicaÃ§Ã£o.

### InstalaÃ§Ã£o da AplicaÃ§Ã£o
1. Clone o repositÃ³rio:
```bash
git clone [seu-repositorio]
cd assistente-restricoes-alimentares
```

2. Instale as dependÃªncias:
```bash
pip install -r requirements.txt
```

3. Execute o aplicativo:
```bash
streamlit run app.py
```

## ğŸ® Como Usar

1. **Crie Seu Perfil**
   - Informe seu nome
   - Selecione suas restriÃ§Ãµes alimentares
   - Adicione observaÃ§Ãµes especiais

2. **Interaja com o Assistente**
   - FaÃ§a perguntas sobre alimentos
   - PeÃ§a sugestÃµes de substituiÃ§Ãµes
   - Tire dÃºvidas sobre ingredientes

3. **Explore os Recursos**
   - Acesse dicas de seguranÃ§a
   - Consulte links Ãºteis
   - Mantenha-se informado

## ğŸ“ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a GNU GPL v3.0, que permite total liberdade para usar, estudar, modificar e distribuir o software e suas versÃµes modificadas. A licenÃ§a garante que essas liberdades sejam preservadas em versÃµes modificadas do software.

## ğŸ™ Agradecimentos

- Ã€ comunidade Streamlit
- Ã€ equipe da Meta pelo excelente trabalho em tornar LLMs acessÃ­veis localmente
